{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
        "\n",
        "Ans:\n",
        "Web scraping is the process of extracting data from websites.\n",
        "It is used to collect information that is not otherwise available in a structured format\n",
        "Three areas where web scraping is used to get data:\n",
        "1. Social media:\n",
        "Web scraping can be used to collect data from social media platforms, such as Twitter, Facebook, and LinkedIn.\n",
        "This data can be used to track trends, to identify influencers, and to conduct market research.\n",
        "2. E-commerce:\n",
        "Web scraping can be used to collect data from e-commerce websites, such as Amazon, flipkart, etc.\n",
        "This data can be used to track prices, to identify trends, and to generate leads, reviews on product.\n",
        "3. Government:\n",
        "Web scraping can be used to collect data from government websites, such as census data, financial data, and environmental data.\n",
        "This data can be used to track trends, to make informed decisions, and to improve public services.\n"
      ],
      "metadata": {
        "id": "PfS1eh5gkl-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the different methods used for Web Scraping?\n",
        "\n",
        "Ans :\n",
        "There are many different methods used for web scraping in Python. Some of the most popular methods include:\n",
        "\n",
        "1. Beautiful Soup: Beautiful Soup is a Python library for parsing HTML and XML documents. It can be used to extract data from websites in a variety of ways, including by using regular expressions, XPath, and CSS selectors.\n",
        "2. Requests: Requests is a Python library for making HTTP requests. It can be used to fetch web pages from websites and to extract data from those web pages.\n",
        "3. Selenium: Selenium is a Python library for automating web browsers. It can be used to interact with websites in a variety of ways, including by clicking buttons, filling out forms, and submitting data.\n",
        "4. Scrapy: Scrapy is a Python framework for web scraping. It provides a number of features that make it easy to scrape websites, including a built-in parser, a scheduler, and a pipeline for storing data."
      ],
      "metadata": {
        "id": "-PLk_tLTk8ZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is Beautiful Soup? Why is it used?\n",
        "\n",
        "Ans: Beautiful Soup is a Python library for parsing HTML and XML documents. It can be used to extract data from websites in a variety of ways, including by using regular expressions, XPath, and CSS selectors.\n",
        "\n",
        "Beautiful Soup is a powerful tool that can be used for a variety of purposes, including:\n",
        "\n",
        "1. Web scraping: Beautiful Soup can be used to extract data from websites. This data can be used for a variety of purposes, such as market research, price monitoring, and lead generation.\n",
        "2. Data cleaning: Beautiful Soup can be used to clean up HTML and XML documents. This can be useful for removing unwanted elements, such as ads and social media buttons.\n",
        "3. Data validation: Beautiful Soup can be used to validate HTML and XML documents. This can be useful for ensuring that the documents are well-formed and that they contain the correct data.\n",
        "4. Web development: Beautiful Soup can be used to debug HTML and XML documents. This can be useful for finding errors in the documents and for improving the documents' performance.\n"
      ],
      "metadata": {
        "id": "Wmwl9BqolbkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.google.com/'\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "title = soup.title\n",
        "print(title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE-HwdL1mG4L",
        "outputId": "bba4bdd1-47a3-448b-eedc-720169aebc67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<title>Google</title>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Why is flask used in this Web Scraping project?\n",
        "\n",
        "Ans :\n",
        "\n",
        "\n",
        "Flask is a microframework for Python web development. It is lightweight and easy to use, making it a good choice for web scraping projects. Flask also provides a number of features that can be useful for web scraping, such as:\n",
        "\n",
        "HTTP requests: Flask can be used to make HTTP requests to websites, which is necessary for web scraping.\n",
        "JSON: Flask can be used to parse JSON data, which is a common format for web scraping data.\n",
        "Templates: Flask can be used to create templates, which can be used to display the scraped data in a user-friendly way.\n"
      ],
      "metadata": {
        "id": "Ydr4BhBpmpf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
        "\n",
        "Ans : In the web scrapping project 2 AWS services were used:\n",
        "1. Code Pipeline\n",
        "2. Elastic Beanstalk\n",
        "\n",
        "* Code Pipeline\n",
        "\n",
        "  1. Code Pipeline is a continuous delivery service that automates the process of building, testing, and deploying your code changes. It can be used to deploy code to a variety of AWS services, including Elastic Beanstalk, EC2, and S3.\n",
        "\n",
        "  2. Code Pipeline works by creating a pipeline, which is a series of stages that your code must go through before it is deployed. Each stage can be configured to perform a specific task, such as building your code, running tests, or deploying your code to a production environment.\n",
        "\n",
        "  3. Code Pipeline can be used to automate the deployment of your code, which can save you time and effort. It can also help to ensure that your code is deployed in a consistent and reliable way.\n",
        "\n",
        "* Elastic Beanstalk\n",
        "\n",
        "  1. Elastic Beanstalk is a service that makes it easy to deploy and manage web applications and services. It provides a managed environment for your applications, which means that you don't have to worry about provisioning or managing servers.\n",
        "\n",
        "  2. Elastic Beanstalk can be used to deploy applications written in a variety of languages, including Java, Python, PHP, and Ruby. It also supports a variety of frameworks, such as Spring Boot, Django, and Rails.\n",
        "\n",
        "  3. Elastic Beanstalk can be used to deploy applications to a variety of AWS services, including EC2, S3, and RDS. It also provides a number of features that can help you to scale your applications, such as load balancing and auto scaling.\n"
      ],
      "metadata": {
        "id": "ITUnZggQm6Sk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ooKruALdnxg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}